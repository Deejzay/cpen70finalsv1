# -*- coding: utf-8 -*-
"""Group 1 - MC02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xto7YS4g3G5Bdok_QlzYdTXmuSUMrPFC

# **DATA COLLECTION**
"""

!pip install streamlit

!npm install localtunnel

from google.colab import files
import io
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns

uploaded = files.upload()
df = pd.read_csv(io.BytesIO(uploaded['Spreadmeat_WQxVAxMF.csv']))

"""# **DATA PRE-PROCESSING**"""

# Dataset Preview and Information
print("Dataset Preview:")
print(df.head())
print("\nDataset Information:")

df.info()

""" 1. Clean missing values (dropna() or fillna())


"""

from sklearn.impute import KNNImputer

# Calculate the number of null values in each row
null_counts_per_row = df.isnull().sum(axis=1)

# Remove rows with no datas frome the df
df_new = df[null_counts_per_row < 10]

# Impute the remaining null values
imputer = KNNImputer(n_neighbors=5)
float_cols = df_new.select_dtypes(include=['float']).columns
df_new[float_cols] = imputer.fit_transform(df_new[float_cols])

"""2. Remove duplicates (drop_duplicates())"""

# Remove duplicates
df.drop_duplicates(inplace=True)

"""3. Convert date-time formats (pd.to_datetime())"""

# Change the date format into 'YEAR-MONTH-DAY'
df_new['Month'] = df_new['Month'].astype(str)
df_new['Month'] = df_new['Month'].str.capitalize()
df_new['Month'] = pd.to_datetime(df_new['Month'], format='%B').dt.month

# Create Date column
df_new['Date'] = pd.to_datetime(df_new[['Year', 'Month']].assign(DAY=1))

# Rearrange the dataframe
cols = list(df_new.columns)
col_to_move = cols.pop()
cols.insert(1, col_to_move)
df_new = df_new[cols]

# Remove Month and Year columns
df_new_unstandardize = df_new.drop(columns=['Month', 'Year'])
df_new = df_new.drop(columns=['Month', 'Year'])

"""4. Normalize values (MinMaxScaler)"""

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Select the columns you want to normalize (e.g., all numeric columns)
numeric_cols = df_new.select_dtypes(include=['number']).columns

# Fit the scaler to the selected columns and transform them
df_new[numeric_cols] = scaler.fit_transform(df_new[numeric_cols])

print("Pre-processed Dataset:")
print(df_new.head())
print("\nDataset Information:")

df_new.info()

"""#**EXPLORATORY DATA ANALYSIS (EDA)**

1. Generate summary statistics (df.describe())
"""

df_new.describe()

"""2. Compute correlation matrix (df.corr())"""

import seaborn as sns
import matplotlib.pyplot as plt
# Remove non-numeric temporarily
numeric_df = df.select_dtypes(include=['number'])

# Generate Correlation matrix
correlation_matrix = numeric_df.corr()

print(correlation_matrix)

"""3. Visualize trends:

## **Line charts for time-series data**
"""

# Set 'Date' as the index if it's not already
if not isinstance(df_new.index, pd.DatetimeIndex):
    df_new = df_new.set_index('Date')

# Unique color list (extend if you have more variables)
colors = ['blue', 'green', 'red', 'purple', 'orange', 'black', 'magenta', 'brown', 'violet', 'gray']

# Get numeric columns
numeric_columns = df_new.select_dtypes(include=np.number).columns

# Safety check: extend colors if needed
if len(numeric_columns) > len(colors):
    from itertools import cycle
    color_cycle = cycle(colors)
    assigned_colors = [next(color_cycle) for _ in range(len(numeric_columns))]
else:
    assigned_colors = colors[:len(numeric_columns)]

# Plot each column with a unique color
for column, color in zip(numeric_columns, assigned_colors):
    plt.figure(figsize=(10, 6))
    plt.plot(df_new.index, df_new[column], color=color)
    plt.xlabel("Date")
    plt.ylabel(column)
    plt.title(f"Time Series Plot of {column}")
    plt.grid(True)

    # Format the x-axis for yearly ticks
    plt.gca().xaxis.set_major_locator(mdates.YearLocator())
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))

    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""## **Heatmaps for feature correlations**"""

# Create a heatmap for visualization
plt.figure(figsize=(12, 10))  # Adjust figure size if needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix Heatmap')
plt.show()

"""## **Scatter plots for parameter relationships**

## **Effects of Environmental Factors in the Water Quality**

## Air Temperature Relationships to
"""

# prompt: scatter graph for air temp, surface temp, air temperature and dissolved oxygen, wind direction and dissolved oxygen, label it as effect of environmental factors to the water quality

# Create scatter plots

# Surface Temperature
plt.figure(figsize=(10, 6))
plt.scatter(df_new['Air Temperature'], df_new['Surface Temp'], label='Air Temperature vs Surface Temperature', alpha=0.7)
plt.xlabel("Air Temperature ")
plt.ylabel("Surface Temperature")
plt.title("Effect of Environmental Factors on Water Quality")
plt.legend()
plt.grid(True)
plt.show()

# Dissolved Oxygen
plt.figure(figsize=(10, 6))
plt.scatter(df_new['Air Temperature'], df_new['Dissolved Oxygen'], label='Air Temperature vs Dissolved Oxygen', alpha=0.7)
plt.xlabel("Air Temperature ")
plt.ylabel("Dissolved Oxygen")
plt.title("Effect of Environmental Factors on Water Quality")
plt.legend()
plt.grid(True)
plt.show()

"""## Wind Direction Relationship to"""

# Create scatter plots for Wind Direction vs. Ammonia, Nitrate, Phosphate, Nitrite
plt.figure(figsize=(14, 12))

plt.subplot(3, 2, 1)
plt.scatter(df_new['Wind Direction'], df_new['Ammonia'], label='Wind Direction vs Ammonia', alpha=0.7)
plt.xlabel("Wind Direction")
plt.ylabel("Ammonia")
plt.title("Wind Direction vs Ammonia")
plt.legend()
plt.grid(True)

plt.subplot(3, 2, 2)
plt.scatter(df_new['Wind Direction'], df_new['Nitrate'], label='Wind Direction vs Nitrate', alpha=0.7)
plt.xlabel("Wind Direction")
plt.ylabel("Nitrate")
plt.title("Wind Direction vs Nitrate")
plt.legend()
plt.grid(True)

plt.subplot(3, 2, 3)
plt.scatter(df_new['Wind Direction'], df_new['Phosphate'], label='Wind Direction vs Phosphate', alpha=0.7)
plt.xlabel("Wind Direction")
plt.ylabel("Phosphate")
plt.title("Wind Direction vs Phosphate")
plt.legend()
plt.grid(True)

plt.subplot(3, 2, 4)
plt.scatter(df_new['Wind Direction'], df_new['Nitrite'], label='Wind Direction vs Nitrite', alpha=0.7)
plt.xlabel("Wind Direction")
plt.ylabel("Nitrite")
plt.title("Wind Direction vs Nitrite")
plt.legend()
plt.grid(True)

# ➡️ NEW scatter plot for Wind Direction vs Dissolved Oxygen
plt.subplot(3, 2, 5)
plt.scatter(df_new['Wind Direction'], df_new['Dissolved Oxygen'], label='Wind Direction vs Dissolved Oxygen', alpha=0.7)
plt.xlabel("Wind Direction")
plt.ylabel("Dissolved Oxygen")
plt.title("Wind Direction vs Dissolved Oxygen")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""## Weather Condition Relationship to"""

import matplotlib.pyplot as plt
import seaborn as sns

weather_params = ['pH', 'Dissolved Oxygen', 'Nitrate', 'Nitrite', 'Ammonia', 'Phosphate']

# Ensure Weather Condition is categorical and sorted alphabetically
df_new['Weather Condition'] = df_new['Weather Condition'].astype(str)
sorted_conditions = sorted(df_new['Weather Condition'].unique())
df_new['Weather Condition'] = pd.Categorical(df_new['Weather Condition'], categories=sorted_conditions, ordered=True)

for param in weather_params:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='Weather Condition', y=param, data=df_new)
    plt.xlabel('Weather Condition')
    plt.ylabel(param)
    plt.title(f'Weather Condition vs. {param}')
    plt.legend([], [], frameon=False)  # remove the legend
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

"""## **Effects of Volcanic Activity to the Water Quality**"""

# Create scatter plots for specific parameter relationships
## CO2 may decrease pH = more acidic = bad
plt.figure(figsize=(12, 8))
plt.scatter(df_new['Carbon Dioxide'], df_new['pH'], label='Carbon Dioxide vs pH', alpha=0.7)
plt.xlabel("Carbon Dioxide")
plt.ylabel("pH")
plt.title("Carbon Dioxide vs pH")
plt.legend()
plt.grid(True)
plt.show()

## CO2 may decrease Dissolved Oxygen = bad
plt.figure(figsize=(12, 8))
plt.scatter(df_new['Carbon Dioxide'], df_new['Dissolved Oxygen'], label='Carbon Dioxide vs Dissolved Oxygen', alpha=0.7)
plt.xlabel("Carbon Dioxide")
plt.ylabel("Dissolved Oxygen")
plt.title("Carbon Dioxide vs Dissolved Oxygen")
plt.legend()
plt.grid(True)
plt.show()

## Sulfide may decrease DO = bad
plt.figure(figsize=(12, 8))
plt.scatter(df_new['Sulfide'], df_new['Dissolved Oxygen'], label='Sulfide vs Dissolved Oxygen', alpha=0.7)
plt.xlabel("Sulfide")
plt.ylabel("Dissolved Oxygen")
plt.title("Sulfide vs Dissolved Oxygen")
plt.legend()
plt.grid(True)
plt.show()

## Sulfide may decrease pH = bad
plt.figure(figsize=(12, 8))
plt.scatter(df_new['Sulfide'], df_new['pH'], label='Sulfide vs pH', alpha=0.7)
plt.xlabel("Sulfide")
plt.ylabel("pH")
plt.title("Sulfide vs pH")
plt.legend()
plt.grid(True)
plt.show()

"""# **Predictive Modeling**

# **PREDICTIVE MODELING USING ENSEMBLE LEARNING***

1. Choose a target variable (e.g., pH or Dissolved Oxygen)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense


# Identify the dependent/target variables to predict
y_pH = df_new['pH']
y_DO = df_new['Dissolved Oxygen']

"""2. Select features (temperature, rainfall, SO₂ levels, etc.)

"""

from sklearn.preprocessing import OneHotEncoder

# Identify the indepedent/predictor variables
X = df_new[['Surface Temp', 'Middle Temp', 'Bottom Temp', 'Nitrite', 'Nitrate',
            'Ammonia', 'Phosphate', 'Sulfide', 'Carbon Dioxide','Air Temperature',
            'Wind Direction', 'Weather Condition']]
# Water Parameters Predictor
X_WP = df_new[['Surface Temp', 'Middle Temp', 'Bottom Temp', 'Nitrite', 'Nitrate',
               'Ammonia', 'Phosphate']]

# Converting Categorical Features into Numerical
X_num = ['Surface Temp', 'Middle Temp', 'Bottom Temp', 'Nitrite', 'Nitrate',
          'Ammonia', 'Phosphate', 'Sulfide', 'Carbon Dioxide', 'Air Temperature']
X_cat = ['Wind Direction', 'Weather Condition']

# Encode categorical variables into numerical
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_cat_data = encoder.fit_transform(df_new[X_cat])
encoded_cat_cols = [f"{col}_{val}" for col in X_cat for val in encoder.categories_[X_cat.index(col)]]
encoded_cat_df = pd.DataFrame(encoded_cat_data, columns=encoded_cat_cols, index=df_new.index)

# Concatenate Numerical and Encoded Categorical
X = pd.concat([df_new[X_num], encoded_cat_df], axis=1)
X.head()

"""3. Train and compare three models:

## **WATER PARAMETERS ONLY**

### **DATA SPLITTING (Water Parameters)**
"""

# Split the data into 60% (Training Set) & 40% Remaining data
X_train, X_rem, y_pH_train, y_pH_rem, y_DO_train, y_DO_rem = train_test_split(X_WP, y_pH, y_DO, test_size=0.4, random_state=42)

# Split the Remaining data into 20% (Test Set) & 20% (Validation Set)
X_valid, X_test, y_pH_valid, y_pH_test, y_DO_valid, y_DO_test = train_test_split(X_rem, y_pH_rem, y_DO_rem, test_size=0.5, random_state=42)

"""### **CNN MODEL (Water Parameters)**"""

# Reshape features shape for CNN
X = np.array(X).reshape(X.shape[0], X.shape[1], 1)

# Build CNN Model
CNN_model = Sequential()
CNN_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
CNN_model.add(MaxPooling1D(pool_size=2))
CNN_model.add(Flatten())
CNN_model.add(Dense(128, activation='relu'))
CNN_model.add(Dense(1, activation='linear'))  # Output layer with 1 unit for DO

# Train CNN Model with Training Set (Dissolved Oxygen)
CNN_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
CNN_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (Dissolved Oxygen)
CNN_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Evaluate DO model
y_DO_pred = CNN_model.predict(X_test)
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'CNN_DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train CNN Model with Training Set (pH Level)
CNN_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
CNN_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
CNN_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = CNN_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'CNN_pH Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""### **LSTM MODEL (Water Parameters)**"""

from tensorflow.keras.layers import LSTM, Dropout

# Build LSTM Model
LSTM_model = Sequential()
LSTM_model.add(LSTM(64, activation='tanh', input_shape=(X_train.shape[1], 1)))
LSTM_model.add(Dense(128, activation='relu'))
LSTM_model.add(Dense(1, activation='linear'))  # Output layer for DO

# Compile
LSTM_model.compile(loss='mse', optimizer='adam')

# Train LSTM Model (Dissolved Oxygen)
LSTM_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))
LSTM_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

# Evaluate DO model
y_DO_pred = LSTM_model.predict(X_test)  #
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train LSTM Model with Training Set (pH Level)
LSTM_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
LSTM_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
LSTM_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = LSTM_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'DO Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""### **CNN-LSTM MODEL (Water Parameters)**"""

# Build Hybrid CNN-LSTM Model
cnn_lstm_model = Sequential()
cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
cnn_lstm_model.add(MaxPooling1D(pool_size=2))
cnn_lstm_model.add(LSTM(64, activation='tanh'))  # LSTM after CNN
cnn_lstm_model.add(Dense(128, activation='relu'))
cnn_lstm_model.add(Dense(1, activation='linear'))  # Output layer for DO

# Compile
cnn_lstm_model.compile(loss='mse', optimizer='adam')

# Train Hybrid Model (Dissolved Oxygen)
cnn_lstm_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))
cnn_lstm_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

# Evaluate DO model
from sklearn.metrics import mean_absolute_error, mean_squared_error
y_DO_pred = cnn_lstm_model.predict(X_test)  #
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train CNN-LSTM Model with Training Set (pH Level)
cnn_lstm_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
cnn_lstm_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
cnn_lstm_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = cnn_lstm_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'DO Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""## **WATER PARAMETERS & EXTERNAL FACTORS**

### **DATA SPLITTING (Water Parameters + Environmental)**
"""

# Split the data into 60% (Training Set) & 40% Remaining data
X_train, X_rem, y_pH_train, y_pH_rem, y_DO_train, y_DO_rem = train_test_split(X, y_pH, y_DO, test_size=0.4, random_state=42)

# Split the Remaining data into 20% (Test Set) & 20% (Validation Set)
X_valid, X_test, y_pH_valid, y_pH_test, y_DO_valid, y_DO_test = train_test_split(X_rem, y_pH_rem, y_DO_rem, test_size=0.5, random_state=42)

"""### **CNN MODEL (Water Parameters + Environmental)**"""

# Reshape features shape for CNN
X = np.array(X).reshape(X.shape[0], X.shape[1], 1)

# Build CNN Model
CNN_model = Sequential()
CNN_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
CNN_model.add(MaxPooling1D(pool_size=2))
CNN_model.add(Flatten())
CNN_model.add(Dense(128, activation='relu'))
CNN_model.add(Dense(1, activation='linear'))  # Output layer with 1 unit for DO

# Train CNN Model with Training Set (Dissolved Oxygen)
CNN_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
CNN_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (Dissolved Oxygen)
CNN_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

# Evaluate DO model
y_DO_pred = CNN_model.predict(X_test)
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'CNN_DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train CNN Model with Training Set (pH Level)
CNN_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
CNN_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
CNN_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = CNN_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'CNN_pH Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""### **LSTM MODEL (Water Parameters + Environmental)**"""

from tensorflow.keras.layers import LSTM, Dropout

# Build LSTM Model
LSTM_model = Sequential()
LSTM_model.add(LSTM(64, activation='tanh', input_shape=(X_train.shape[1], 1)))
LSTM_model.add(Dense(128, activation='relu'))
LSTM_model.add(Dense(1, activation='linear'))

# Compile
LSTM_model.compile(loss='mse', optimizer='adam')

# Train LSTM Model (Dissolved Oxygen)
LSTM_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))
LSTM_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

# Evaluate DO model
y_DO_pred = LSTM_model.predict(X_test)  #
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train LSTM Model with Training Set (pH Level)
LSTM_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
LSTM_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
LSTM_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = LSTM_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'pH Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""### **CNN-LSTM MODEL (Water Parameters + Environmental)**"""

# Build Hybrid CNN-LSTM Model
cnn_lstm_model = Sequential()
cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
cnn_lstm_model.add(MaxPooling1D(pool_size=2))
cnn_lstm_model.add(LSTM(64, activation='tanh'))  # LSTM after CNN
cnn_lstm_model.add(Dense(128, activation='relu'))
cnn_lstm_model.add(Dense(1, activation='linear'))  # Output layer for DO

# Compile
cnn_lstm_model.compile(loss='mse', optimizer='adam')

# Train Hybrid Model (Dissolved Oxygen)
cnn_lstm_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))
cnn_lstm_model.fit(X_train, y_DO_train, epochs=100, batch_size=32, validation_data=(X_valid, y_DO_valid))

# Evaluate DO model
y_DO_pred = cnn_lstm_model.predict(X_test)  #
mae_DO = mean_absolute_error(y_DO_test, y_DO_pred)
rmse_DO = np.sqrt(mean_squared_error(y_DO_test, y_DO_pred))
print(f'DO Model - MAE: {mae_DO}, RMSE: {rmse_DO}')

# Train CNN-LSTM Model with Training Set (pH Level)
cnn_lstm_model.compile(loss='mse', optimizer='adam')  # Use appropriate loss and optimizer
cnn_lstm_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_test, y_DO_test))

# Train the model with Validation Set (pH Level)
cnn_lstm_model.fit(X_train, y_pH_train, epochs=100, batch_size=32, validation_data=(X_valid, y_pH_valid))

# Evaluate pH model
y_pH_pred = cnn_lstm_model.predict(X_test)
mae_pH = mean_absolute_error(y_pH_test, y_pH_pred)
rmse_pH = np.sqrt(mean_squared_error(y_pH_test, y_pH_pred))
print(f'pH Model - MAE: {mae_pH}, RMSE: {rmse_pH}')

"""# **PREDICTION BASED ON TIME**


"""



"""### **DATA PRE-PROCESSING**"""

# Dataset Preview and Information
print("Dataset Preview:")
print(df.head())
print("\nDataset Information:")

df.info()

""" 1. Clean missing values (dropna() or fillna())


"""

from sklearn.impute import KNNImputer

# Calculate the number of null values in each row
null_counts_per_row = df.isnull().sum(axis=1)

# Remove rows with no datas frome the df
df_new = df[null_counts_per_row < 10]

# Impute the remaining null values
imputer = KNNImputer(n_neighbors=5)
float_cols = df_new.select_dtypes(include=['float']).columns
df_new[float_cols] = imputer.fit_transform(df_new[float_cols])

"""2. Remove duplicates (drop_duplicates())"""

# Remove duplicates
df.drop_duplicates(inplace=True)

"""3. Convert date-time formats (pd.to_datetime())"""

# Change the date format into 'YEAR-MONTH-DAY'
df_new['Month'] = df_new['Month'].astype(str)
df_new['Month'] = df_new['Month'].str.capitalize()
df_new['Month'] = pd.to_datetime(df_new['Month'], format='%B').dt.month

# Create Date column
df_new['Date'] = pd.to_datetime(df_new[['Year', 'Month']].assign(DAY=1))

# Rearrange the dataframe
cols = list(df_new.columns)
col_to_move = cols.pop()
cols.insert(1, col_to_move)
df_new = df_new[cols]

# Remove Month and Year columns
df_new_unstandardize = df_new.drop(columns=['Month', 'Year'])
df_new = df_new.drop(columns=['Month', 'Year'])

"""4. Normalize values (MinMaxScaler)"""

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Select the columns you want to normalize (e.g., all numeric columns)
numeric_cols = df_new.select_dtypes(include=['number']).columns

# Fit the scaler to the selected columns and transform them
df_new[numeric_cols] = scaler.fit_transform(df_new[numeric_cols])

print("Pre-processed Dataset:")
print(df_new.head())
print("\nDataset Information:")

df_new.info()

"""## **CNN**

### Monthly Based Prediction
"""

df_new.info()

import pandas as pd
import numpy as np

location = 'TANAUAN'  # Or any other location in your dataset
location_data = df_new[df_new['Site'] == location].copy()

def prepare_monthly_data(df, location, target_variables, look_back=12):  # Default look_back is 12 months
    location_data = df[df['Site'] == location].copy()
    location_data['Date'] = pd.to_datetime(location_data['Date'])
    location_data = location_data.set_index('Date')

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('M').mean()  # Resample monthly
    location_data = location_data[target_variables]

    # Remove rows with null values
    location_data = location_data.dropna(subset=target_variables)

    # Check if enough data points for lookback
    if len(location_data) <= look_back:
        raise ValueError("Not enough data points for the specified lookback period.")

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - look_back - 1):
        X.append(location_data[target_variables].values[i : i + look_back])
        Y.append(location_data[target_variables].values[i + look_back])
    X = np.array(X)
    Y = np.array(Y)

    count = len(location_data)
    return X, Y, count, location_data  # Return location_data as well

# Example usage:
target_vars = ['Surface Temp', 'Middle Temp', 'Bottom Temp', 'pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']
X, Y, count, location_data = prepare_monthly_data(df_new, location=location, target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Define look_back here
look_back = 12  # Set to the desired lookback period

# Get the number of features from the input data
num_features = X.shape[2]  # X.shape is (num_samples, look_back, num_features)

CNN_model = Sequential()
CNN_model.add(Conv1D(filters=64, kernel_size=3, activation='relu',
                      input_shape=(look_back, num_features))) # Update input_shape
CNN_model.add(MaxPooling1D(pool_size=2))
CNN_model.add(Flatten())
CNN_model.add(Dense(128, activation='relu'))
CNN_model.add(Dense(len(target_vars)))  # Output layer with units for each target variable

CNN_model.compile(loss='mse', optimizer='adam')
CNN_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

# Store predictions for the next 12 months
predictions = []

# Predict for the next 12 months
for i in range(12):  # or any desired prediction horizon
    # Reshape the input data for the model
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))

    # Make a prediction for the next month
    next_month_pred = CNN_model.predict(input_data)[0]  # Assuming 'model' is your trained model

    # Append the prediction to the predictions list
    predictions.append(next_month_pred)

    # Update the last_months_data with the prediction
    last_months_data = np.vstack([last_months_data, next_month_pred])

# Convert predictions to a DataFrame (optional)
predictions_df = pd.DataFrame(predictions, columns=target_vars)


# Print or use the predictions_df as needed
print("Next 12 Months' Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 12 Months' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

# Calculate and print RMSE and MAE for each target variable
for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\nMetrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Monthly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""### Yearly Based Prediction


"""

import pandas as pd
import numpy as np

location_data = df_new[df_new['Site'] == 'AYA'].copy()

def prepare_yearly_data(df, location, target_variables, look_back=5):  # Default look_back is 5 years
    location_data = df[df['Site'] == location].copy()
    location_data['Date'] = pd.to_datetime(location_data['Date'])
    location_data = location_data.set_index('Date')

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('Y').mean()  # Resample yearly
    location_data = location_data[target_variables]

    # Remove rows with null values
    location_data = location_data.dropna(subset=target_variables)

    # Check if enough data points for lookback
    if len(location_data) <= look_back:
        raise ValueError("Not enough data points for the specified lookback period.")

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - look_back - 1):
        X.append(location_data[target_variables].values[i : i + look_back])
        Y.append(location_data[target_variables].values[i + look_back])
    X = np.array(X)
    Y = np.array(Y)

    count = len(location_data)
    return X, Y, count, location_data  # Return location_data as well

# Example usage:
target_vars = [ 'Surface Temp', 'Middle Temp', 'Bottom Temp','pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']
X, Y, count, location_data = prepare_yearly_data(df_new, location='TANAUAN', target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Define look_back here
look_back = 5  # Set to the desired lookback period

# Get the number of features from the input data
num_features = X.shape[2]  # X.shape is (num_samples, look_back, num_features)

CNN_model = Sequential()
CNN_model.add(Conv1D(filters=64, kernel_size=3, activation='relu',
                      input_shape=(look_back, num_features))) # Update input_shape
CNN_model.add(MaxPooling1D(pool_size=2))
CNN_model.add(Flatten())
CNN_model.add(Dense(128, activation='relu'))
CNN_model.add(Dense(len(target_vars)))  # Output layer with units for each target variable

CNN_model.compile(loss='mse', optimizer='adam')
CNN_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler


# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

# Store predictions for the next 12 months
predictions = []

# Predict for the next 12 months
for i in range(5):
    # Reshape the input data for the model
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))

    # Make a prediction for the next month
    next_month_pred = CNN_model.predict(input_data)[0]

    # Append the prediction to the predictions list
    predictions.append(next_month_pred)

    # Update the last_months_data with the prediction
    last_months_data = np.vstack([last_months_data, next_month_pred])

# Convert predictions to a DataFrame (optional)
predictions_df = pd.DataFrame(predictions, columns=target_vars)


# Print or use the predictions_df as needed
print("Next 5 Years Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 5 Years' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\n Metrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Yearly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""## **LSTM**"""

df_new.info()

"""### Monthly Based Prediction"""

import pandas as pd
import numpy as np

location = 'AYA'
location_data = df_new[df_new['Site'] == location].copy()

def prepare_monthly_data(df, location, target_variables):
    location_data = df[df['Site'] == location].copy()  # Filter data for the location
    location_data['Date'] = pd.to_datetime(location_data['Date'])  # Ensure Date is in datetime format
    location_data = location_data.set_index('Date')  # Set Date as index

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('M').mean()  # Resample and calculate monthly averages for numeric columns
    location_data = location_data[target_variables]  # Select the target variables

    # Remove rows with null values in any of the target variables
    location_data = location_data.dropna(subset=target_variables)

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - 12 - 1):  # Use 12 months lookback
        X.append(location_data[target_variables].values[i : i + 12])  # Input features
        Y.append(location_data[target_variables].values[i + 12])  # Target variables
    X = np.array(X)
    Y = np.array(Y)

    # Get the count of data points after removing null values
    count = len(location_data)

    return X, Y, count

# Example usage with multiple target variables:
target_vars = [ 'Surface Temp', 'Middle Temp', 'Bottom Temp','pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']   # List of target variables
X, Y, count = prepare_monthly_data(df_new, location=location, target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Build LSTM Model
look_back = 12  # 12 months lookback
num_features = X.shape[2]  # Number of target variables

LSTM_model = Sequential()
LSTM_model.add(LSTM(64, activation='tanh', input_shape=(look_back, num_features)))
LSTM_model.add(Dense(128, activation='relu'))
LSTM_model.add(Dense(len(target_vars)))  # Output layer for all target variables

LSTM_model.compile(loss='mse', optimizer='adam')
LSTM_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler


# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

predictions = []

for i in range(12):  # Predict for the next 12 months
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))
    next_month_pred = LSTM_model.predict(input_data)[0]
    predictions.append(next_month_pred)
    last_months_data = np.vstack([last_months_data, next_month_pred])

predictions_df = pd.DataFrame(predictions, columns=target_vars)

# Print or use the predictions_df as needed
print("Next 12 Months' Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 12 Months' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\n Metrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Monthly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""### Yearly Based Prediction


"""

import pandas as pd
import numpy as np

location = 'TANAUAN'
location_data = df_new[df_new['Site'] == location].copy()

def prepare_yearly_data(df, location, target_variables, look_back=5):  # Default look_back is 5 years
    location_data = df[df['Site'] == location].copy()
    location_data['Date'] = pd.to_datetime(location_data['Date'])
    location_data = location_data.set_index('Date')

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('Y').mean()  # Resample yearly
    location_data = location_data[target_variables]

    # Remove rows with null values
    location_data = location_data.dropna(subset=target_variables)

    # Check if enough data points for lookback
    if len(location_data) <= look_back:
        raise ValueError("Not enough data points for the specified lookback period.")

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - look_back - 1):
        X.append(location_data[target_variables].values[i : i + look_back])
        Y.append(location_data[target_variables].values[i + look_back])
    X = np.array(X)
    Y = np.array(Y)

    count = len(location_data)
    return X, Y, count, location_data  # Return location_data as well

# Example usage:
target_vars = [ 'Surface Temp', 'Middle Temp', 'Bottom Temp','pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']
X, Y, count, location_data = prepare_yearly_data(df_new, location=location, target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Build LSTM Model
look_back = 5  # 5 years lookback
num_features = X.shape[2]  # Number of target variables

LSTM_model = Sequential()
LSTM_model.add(LSTM(64, activation='tanh', input_shape=(look_back, num_features)))
LSTM_model.add(Dense(128, activation='relu'))
LSTM_model.add(Dense(len(target_vars)))  # Output layer for all target variables

LSTM_model.compile(loss='mse', optimizer='adam')
LSTM_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler


# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

# Store predictions for the next 12 months
predictions = []

# Predict for the next 12 months
for i in range(5):
    # Reshape the input data for the model
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))

    # Make a prediction for the next month
    next_month_pred = LSTM_model.predict(input_data)[0]

    # Append the prediction to the predictions list
    predictions.append(next_month_pred)

    # Update the last_months_data with the prediction
    last_months_data = np.vstack([last_months_data, next_month_pred])

# Convert predictions to a DataFrame (optional)
predictions_df = pd.DataFrame(predictions, columns=target_vars)


# Print or use the predictions_df as needed
print("Next 5 Years' Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 5 Years' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\n Metrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Yearly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""## **CNN-LSTM**"""

df_new.info()

"""### Monthly Based Prediction"""

import pandas as pd
import numpy as np

location_data = df_new[df_new['Site'] == 'AYA'].copy()

def prepare_monthly_data(df, location, target_variables):
    location_data = df[df['Site'] == location].copy()  # Filter data for the location
    location_data['Date'] = pd.to_datetime(location_data['Date'])  # Ensure Date is in datetime format
    location_data = location_data.set_index('Date')  # Set Date as index

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('M').mean()  # Resample and calculate monthly averages for numeric columns
    location_data = location_data[target_variables]  # Select the target variables

    # Remove rows with null values in any of the target variables
    location_data = location_data.dropna(subset=target_variables)

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - 12 - 1):  # Use 12 months lookback
        X.append(location_data[target_variables].values[i : i + 12])  # Input features
        Y.append(location_data[target_variables].values[i + 12])  # Target variables
    X = np.array(X)
    Y = np.array(Y)

    # Get the count of data points after removing null values
    count = len(location_data)

    return X, Y, count

# Example usage with multiple target variables:
target_vars = [ 'Surface Temp', 'Middle Temp', 'Bottom Temp','pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']  # List of target variables
X, Y, count = prepare_monthly_data(df_new, location='AYA', target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense


# Define look_back here
look_back = 12  # Set to the desired lookback period (e.g., 12 months)

# Get the number of features from the input data
num_features = X.shape[2]  # X.shape is (num_samples, look_back, num_features)


# Build the CNN-LSTM model
CNN_LSTM_model = Sequential()
CNN_LSTM_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, num_features)))
CNN_LSTM_model.add(MaxPooling1D(pool_size=2))
CNN_LSTM_model.add(LSTM(64, activation='tanh'))  # Add the LSTM layer
CNN_LSTM_model.add(Dense(128, activation='relu'))
CNN_LSTM_model.add(Dense(len(target_vars)))  # Output layer with units for each target variable

CNN_LSTM_model.compile(loss='mse', optimizer='adam')
CNN_LSTM_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler


# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

# Store predictions for the next 12 months
predictions = []

# Predict for the next 12 months
for i in range(12):
    # Reshape the input data for the model
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))

    # Make a prediction for the next month
    next_month_pred = CNN_LSTM_model.predict(input_data)[0]

    # Append the prediction to the predictions list
    predictions.append(next_month_pred)

    # Update the last_months_data with the prediction
    last_months_data = np.vstack([last_months_data, next_month_pred])

# Convert predictions to a DataFrame (optional)
predictions_df = pd.DataFrame(predictions, columns=target_vars)


# Print or use the predictions_df as needed
print("Next 12 Months' Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 12 Months' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\n Metrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Monthly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""### Yearly Based Prediction


"""

import pandas as pd
import numpy as np

location_data = df_new[df_new['Site'] == 'AYA'].copy()

def prepare_yearly_data(df, location, target_variables, look_back=5):  # Default look_back is 5 years
    location_data = df[df['Site'] == location].copy()
    location_data['Date'] = pd.to_datetime(location_data['Date'])
    location_data = location_data.set_index('Date')

    # Select only numeric columns before resampling
    numeric_location_data = location_data.select_dtypes(include=['number'])

    location_data = numeric_location_data.resample('Y').mean()  # Resample yearly
    location_data = location_data[target_variables]

    # Remove rows with null values
    location_data = location_data.dropna(subset=target_variables)

    # Check if enough data points for lookback
    if len(location_data) <= look_back:
        raise ValueError("Not enough data points for the specified lookback period.")

    # Create input features (X) and target variable (Y)
    X, Y = [], []
    for i in range(len(location_data) - look_back - 1):
        X.append(location_data[target_variables].values[i : i + look_back])
        Y.append(location_data[target_variables].values[i + look_back])
    X = np.array(X)
    Y = np.array(Y)

    count = len(location_data)
    return X, Y, count, location_data  # Return location_data as well

# Example usage:
target_vars = [ 'Surface Temp', 'Middle Temp', 'Bottom Temp','pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate']
X, Y, count, location_data = prepare_yearly_data(df_new, location='TANAUAN', target_variables=target_vars)

print(f"Number of data points after removing null values: {count}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense

# Define look_back here
look_back = 5  # Set to the desired lookback period

# Get the number of features from the input data
num_features = X.shape[2]  # X.shape is (num_samples, look_back, num_features)

# Build the CNN-LSTM model
CNN_LSTM_model = Sequential()
CNN_LSTM_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, num_features)))
CNN_LSTM_model.add(MaxPooling1D(pool_size=2))
CNN_LSTM_model.add(LSTM(64, activation='tanh'))  # Add the LSTM layer
CNN_LSTM_model.add(Dense(128, activation='relu'))
CNN_LSTM_model.add(Dense(len(target_vars)))  # Output layer with units for each target variable

CNN_LSTM_model.compile(loss='mse', optimizer='adam')
CNN_LSTM_model.fit(X, Y, epochs=100, batch_size=32)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.preprocessing import MinMaxScaler


# Get the last 'look_back' months' data as initial input
last_months_data = location_data[target_vars].values[-look_back:]

# Store predictions for the next 12 months
predictions = []

# Predict for the next 12 months
for i in range(5):
    # Reshape the input data for the model
    input_data = last_months_data[-look_back:].reshape(1, look_back, len(target_vars))

    # Make a prediction for the next month
    next_month_pred = CNN_LSTM_model.predict(input_data)[0]

    # Append the prediction to the predictions list
    predictions.append(next_month_pred)

    # Update the last_months_data with the prediction
    last_months_data = np.vstack([last_months_data, next_month_pred])

# Convert predictions to a DataFrame (optional)
predictions_df = pd.DataFrame(predictions, columns=target_vars)


# Print or use the predictions_df as needed
print("Next 12 Months' Predictions (STANDARDIZED):")
print(predictions_df)

original_numeric_cols = df_new_unstandardize[target_vars]  # Selecting only the target variables
scaler.fit(original_numeric_cols)  # Fit the scaler to these target variables

descaled_predictors_df = pd.DataFrame(scaler.inverse_transform(predictions_df),
                                         columns=predictions_df.columns, index=predictions_df.index)

print("\n Next 12 Months' Predictions (ORIGINAL UNITS):")
print(descaled_predictors_df)

for column in predictions_df.columns:
    # Get actual and predicted values for the current column
    actual_values = Y[-12:, predictions_df.columns.get_loc(column)]  # Slice Y to get the last 12 months
    predicted_values = predictions_df[column]

    # Calculate RMSE for the current column
    rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))

    # Calculate MAE for the current column
    mae = mean_absolute_error(actual_values, predicted_values)

    print(f"\n Metrics for {column}:")
    print(f"  RMSE: {rmse}")
    print(f"  MAE: {mae}")

"""### WQI Calculation Yearly CNN"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)

"""# **WQI CALCULATION YEARLY**"""

def calculate_wqi(df, parameters):
    # Define weights for each parameter (you can adjust these based on your specific needs)
    weights = {
        'pH': 0.15,
        'Dissolved Oxygen': 0.25,
        'Nitrite': 0.10,
        'Nitrate': 0.10,
        'Ammonia': 0.15,
        'Phosphate': 0.10,
        'Surface Temp': 0.05,
        'Middle Temp': 0.05,
        'Bottom Temp': 0.05,
    }

    # Ensure all parameters are in the DataFrame
    missing_params = [param for param in parameters if param not in df.columns]
    if missing_params:
        raise ValueError(f"Missing parameters in DataFrame: {missing_params}")

    # Calculate weighted values for each parameter
    weighted_values = df[parameters].apply(lambda x: x * weights[x.name], axis=0)

    # Calculate WQI by summing the weighted values
    wqi = weighted_values.sum(axis=1)

    return wqi

# Assuming 'descaled_predictors_df' contains your predicted values
wqi_values = calculate_wqi(df, parameters=['pH', 'Dissolved Oxygen', 'Nitrite', 'Nitrate', 'Ammonia', 'Phosphate', 'Surface Temp', 'Middle Temp', 'Bottom Temp'])
descaled_predictors_df['WQI'] = wqi_values

def classify_water_quality(wqi):
  """Classifies water quality based on WQI value."""
  if wqi >= 90:
    return 'Excellent'
  elif wqi >= 70:
    return 'Good'
  elif wqi >= 50:
    return 'Fair'
  elif wqi >= 25:
    return 'Poor'
  else:
    return 'Very Poor'

descaled_predictors_df['Water Quality'] = descaled_predictors_df['WQI'].apply(classify_water_quality)

print(descaled_predictors_df)